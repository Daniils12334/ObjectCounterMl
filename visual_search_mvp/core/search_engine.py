import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from PIL import Image
from typing import List, Tuple, Dict, Any

from config import Config
from models.clip_model import CLIPModel
from core.utils import ImageUtils, ColorAnalyzer

class VisualSearchEngine:
    def __init__(self, config: Config = None):
        self.config = config or Config()
        
        # Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
        self.model = CLIPModel(
            model_name=self.config.MODEL_NAME,
            device=self.config.DEVICE
        )
        
        # Ğ¥Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        self.embeddings = []
        self.metadata = []
        
        print("ğŸš€ Visual Search Engine initialized!")
    
    def index_image(self, image_path: str, image_id: str = None) -> List[tuple]:
        """Ğ˜Ğ½Ğ´ĞµĞºÑĞ¸Ñ€ÑƒĞµĞ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ - ĞĞ¡ĞĞĞ’ĞĞĞ¯ Ğ¤Ğ£ĞĞšĞ¦Ğ˜Ğ¯"""
        print(f"ğŸ“¸ Indexing: {image_path}")
        
        # Ğ¡ĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼
        bboxes = ImageUtils.segment_image(
            image_path, 
            self.config.SEGMENTATION['num_segments']
        )
        
        if not bboxes:
            print("âŒ No segments found")
            return []
        
        # ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ crops
        pil_image = Image.open(image_path).convert('RGB')
        crops = []
        valid_bboxes = []
        
        for i, (x, y, w, h) in enumerate(bboxes):
            try:
                # Ğ’Ñ‹Ñ€ĞµĞ·Ğ°ĞµĞ¼ Ñ Ğ¾Ñ‚ÑÑ‚ÑƒĞ¿Ğ¾Ğ¼
                padding = 5
                x1, y1 = max(0, x-padding), max(0, y-padding)
                x2, y2 = min(pil_image.width, x+w+padding), min(pil_image.height, y+h+padding)
                
                crop = pil_image.crop((x1, y1, x2, y2))
                crops.append(crop)
                valid_bboxes.append((x, y, w, h))
                
            except Exception as e:
                print(f"âš ï¸  Skipping segment {i}: {e}")
                continue
        
        # Ğ‘ĞĞ¢Ğ§ĞĞĞ• Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² - Ğ‘Ğ«Ğ¡Ğ¢Ğ Ğ•Ğ•!
        embeddings_batch = self.model.batch_get_embeddings(crops)
        
        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹
        added_count = 0
        for i, (embedding, bbox) in enumerate(zip(embeddings_batch, valid_bboxes)):
            if embedding is not None:
                self.embeddings.append(embedding)
                self.metadata.append({
                    'image_id': image_id or image_path,
                    'segment_id': len(self.metadata),
                    'bbox': bbox,
                    'image_path': image_path
                })
                added_count += 1
        
        print(f"âœ… Successfully indexed {added_count}/{len(bboxes)} segments")
        return valid_bboxes[:added_count]
    
    def search(self, query_image_path: str, top_k: int = None, threshold: float = None) -> List[tuple]:
        """ĞŸĞ¾Ğ¸ÑĞº Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²"""
        if not self.embeddings:
            print("âŒ No images indexed yet!")
            return []
        
        top_k = top_k or self.config.SEARCH['top_k']
        threshold = threshold or self.config.SEARCH['similarity_threshold']
        
        print(f"ğŸ” Searching: {query_image_path}")
        
        try:
            # Ğ­Ğ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³ Ğ´Ğ»Ñ query
            query_image = Image.open(query_image_path).convert('RGB')
            query_embedding = self.model.get_embedding(query_image)
            
            if query_embedding is None:
                return []
            
            # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚Ğ¸
            similarities = cosine_similarity([query_embedding], self.embeddings)[0]
            
            # Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¸ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµĞ¼
            results = []
            for i, similarity in enumerate(similarities):
                if similarity >= threshold:
                    results.append((self.metadata[i], similarity))
            
            # Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ğ¾ ÑƒĞ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚Ğ¸
            results.sort(key=lambda x: x[1], reverse=True)
            
            print(f"ğŸ¯ Found {len(results)} matches (showing top {top_k})")
            return results[:top_k]
            
        except Exception as e:
            print(f"âŒ Search error: {e}")
            return []
    
    def get_stats(self) -> Dict[str, Any]:
        """Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹"""
        return {
            'total_segments': len(self.embeddings),
            'total_images': len(set(m['image_id'] for m in self.metadata)),
            'embedding_dim': self.embeddings[0].shape[0] if self.embeddings else 0
        }
    
class ImprovedVisualSearchEngine(VisualSearchEngine):
    def __init__(self, config=None):
        super().__init__(config)
        self.color_analyzer = ColorAnalyzer()
    
    def search_with_color_filter(self, query_image_path: str, target_color: str = None, 
                               color_tolerance: int = 20, top_k: int = 5, threshold: float = 0.6) -> List[tuple]:
        """ĞŸĞ¾Ğ¸ÑĞº Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ğ¾ Ñ†Ğ²ĞµÑ‚Ñƒ"""
        
        # Ğ¡Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ´ĞµĞ»Ğ°ĞµĞ¼ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº
        all_results = self.search(query_image_path, top_k=top_k*3, threshold=threshold*0.7)
        
        if not all_results or not target_color:
            return all_results[:top_k]
        
        # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµĞ¼ Ğ¿Ğ¾ Ñ†Ğ²ĞµÑ‚Ñƒ
        filtered_results = []
        
        for metadata, similarity in all_results:
            try:
                # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ²Ñ‹Ñ€ĞµĞ·Ğ°ĞµĞ¼ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚
                img = Image.open(metadata['image_path']).convert('RGB')
                x, y, w, h = metadata['bbox']
                
                # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¾Ñ‚ÑÑ‚ÑƒĞ¿Ñ‹
                padding = 5
                x1, y1 = max(0, x-padding), max(0, y-padding)
                x2, y2 = min(img.width, x+w+padding), min(img.height, y+h+padding)
                
                segment_crop = img.crop((x1, y1, x2, y2))
                
                # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ†Ğ²ĞµÑ‚
                if self.color_analyzer.filter_by_color(segment_crop, target_color, color_tolerance):
                    filtered_results.append((metadata, similarity))
                    
                    if len(filtered_results) >= top_k:
                        break
                        
            except Exception as e:
                print(f"âš ï¸ Color filtering error: {e}")
                continue
        
        print(f"ğŸ¨ After color filtering: {len(filtered_results)}/{len(all_results)} matches")
        return filtered_results
    
    def search_combined(self, query_image_path: str, target_color: str = None,
                    shape_weight: float = 0.7, color_weight: float = 0.3,
                    top_k: int = 5) -> List[tuple]:
        """ĞšĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ñ„Ğ¾Ñ€Ğ¼Ğµ Ğ¸ Ñ†Ğ²ĞµÑ‚Ñƒ"""
        
        # 1. ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³ query Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ (Ñ„Ğ¾Ñ€Ğ¼Ğ°)
        query_image = Image.open(query_image_path).convert('RGB')
        query_embedding = self.model.get_embedding(query_image)
        
        if query_embedding is None:
            return []
        
        # 2. Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚Ğ¸ Ğ¿Ğ¾ Ñ„Ğ¾Ñ€Ğ¼Ğµ
        shape_similarities = cosine_similarity([query_embedding], self.embeddings)[0]
        
        # 3. Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚Ğ¸ Ğ¿Ğ¾ Ñ†Ğ²ĞµÑ‚Ñƒ (ĞµÑĞ»Ğ¸ ÑƒĞºĞ°Ğ·Ğ°Ğ½ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ†Ğ²ĞµÑ‚)
        color_similarities = np.ones(len(self.embeddings))
        
        if target_color:
            for i, metadata in enumerate(self.metadata):
                try:
                    img = Image.open(metadata['image_path']).convert('RGB')
                    x, y, w, h = metadata['bbox']
                    
                    padding = 5
                    x1, y1 = max(0, x-padding), max(0, y-padding)
                    x2, y2 = min(img.width, x+w+padding), min(img.height, y+h+padding)
                    
                    segment_crop = img.crop((x1, y1, x2, y2))
                    
                    # Ğ§ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ñ†Ğ²ĞµÑ‚Ñƒ, Ñ‚ĞµĞ¼ Ğ²Ñ‹ÑˆĞµ score
                    color_match = self.color_analyzer.filter_by_color(segment_crop, target_color, 30)
                    color_similarities[i] = 1.0 if color_match else 0.3
                    
                except:
                    color_similarities[i] = 0.5
        
        # 4. ĞšĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµĞ¼ scores
        combined_scores = (shape_similarities * shape_weight + 
                        color_similarities * color_weight)
        
        # 5. Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¸ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹
        results = []
        for i in np.argsort(combined_scores)[::-1]:
            if combined_scores[i] > 0.5:  # ĞĞ±Ñ‰Ğ¸Ğ¹ Ğ¿Ğ¾Ñ€Ğ¾Ğ³
                results.append((self.metadata[i], combined_scores[i]))
            
            if len(results) >= top_k:
                break
        
        return results