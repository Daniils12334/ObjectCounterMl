models:
  clip:
    name: "ViT-B/32"
    input_size: 224
    embedding_dim: 512
    preprocessing:
      mean: [0.48145466, 0.4578275, 0.40821073]
      std: [0.26862954, 0.26130258, 0.27577711]
  
  dinov2:
    name: "dinov2_vitb14"
    input_size: 224
    embedding_dim: 768
    preprocessing:
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]

search:
  default_model: "clip"
  similarity_threshold: 0.7
  top_k: 10
  use_faiss: true